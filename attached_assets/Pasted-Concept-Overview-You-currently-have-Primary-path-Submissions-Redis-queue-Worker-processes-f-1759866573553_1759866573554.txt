Concept Overview

You currently have:

Primary path: Submissions ‚Üí Redis queue ‚Üí Worker processes (for async handling)

Goal: If Redis is unavailable (network down, service crash, etc.), the system should automatically write to Postgres so that no submission is lost.

So the flow becomes:

Try:
    Push submission ‚Üí Redis
Except RedisError:
    Insert submission ‚Üí Postgres fallback table

üß© Recommended Design Pattern

We‚Äôll follow a try‚Äìexcept fallback mechanism, wrapped in a queue abstraction layer.

1. Create a Queue Interface

Instead of directly writing to Redis everywhere, create a small utility or class that handles enqueuing.

# queue_service.py
import redis
import psycopg2
from psycopg2.extras import Json
import json

class SubmissionQueue:
    def __init__(self, redis_url, pg_conn):
        self.redis = redis.Redis.from_url(redis_url, decode_responses=True)
        self.pg_conn = pg_conn

    def enqueue_submission(self, submission_data: dict):
        try:
            # Try pushing to Redis queue
            self.redis.rpush("submission_queue", json.dumps(submission_data))
            print("‚úÖ Submission queued to Redis")

        except redis.exceptions.RedisError as e:
            print("‚ö†Ô∏è Redis unavailable, falling back to Postgres")
            self._save_to_postgres(submission_data)

    def _save_to_postgres(self, submission_data):
        try:
            cur = self.pg_conn.cursor()
            cur.execute("""
                INSERT INTO fallback_submissions (data, created_at)
                VALUES (%s, NOW())
            """, [Json(submission_data)])
            self.pg_conn.commit()
            cur.close()
            print("‚úÖ Saved submission to Postgres fallback")
        except Exception as e:
            print("‚ùå Failed to save fallback submission:", e)

2. Create a Postgres Table for Fallbacks

You can create a simple JSON table:

CREATE TABLE IF NOT EXISTS fallback_submissions (
    id SERIAL PRIMARY KEY,
    data JSONB NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);


This way, if Redis is down, nothing is lost ‚Äî the submissions stay in this table until your system recovers.

3. Background Recovery Job (Optional but Recommended)

Once Redis comes back online, you can requeue those pending submissions from Postgres ‚Üí Redis.

def recover_fallback_submissions(queue: SubmissionQueue):
    cur = queue.pg_conn.cursor()
    cur.execute("SELECT id, data FROM fallback_submissions ORDER BY id ASC LIMIT 100")
    rows = cur.fetchall()
    
    for row_id, data in rows:
        try:
            queue.redis.rpush("submission_queue", json.dumps(data))
            cur.execute("DELETE FROM fallback_submissions WHERE id = %s", [row_id])
        except redis.exceptions.RedisError:
            print("Redis still down, stopping recovery")
            break
    
    queue.pg_conn.commit()
    cur.close()


You could run this periodically with:

a cron job, or

a background worker (Celery/APS/Thread).

‚öôÔ∏è In a FastAPI Application (Example Integration)
# main.py
from fastapi import FastAPI, HTTPException
from queue_service import SubmissionQueue
import psycopg2

app = FastAPI()

pg_conn = psycopg2.connect(
    host="your_postgres_host",
    dbname="your_db",
    user="your_user",
    password="your_password"
)

queue = SubmissionQueue(redis_url="redis://localhost:6379/0", pg_conn=pg_conn)

@app.post("/submit")
def submit(submission: dict):
    try:
        queue.enqueue_submission(submission)
        return {"message": "Submission received!"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

‚úÖ Advantages

No data loss: Submissions are safely written somewhere (Redis or Postgres).

Automatic recovery: Old submissions can be retried.

Simple logic: Only a few lines of additional code.

Future extensibility: You can easily extend to Kafka, RabbitMQ, or SQS later.